{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "# 'dmsloc', 'ctxcol', 'interdmsctgABBA', '1backloc'\n",
    "# ins_mapping = {\"dmsloc\": 'Do stimulus 1 and 2 match in LOCATION?',\n",
    "#                '1backloc': 'In this task, you will see sequences of six stimuli. \\n From the second stimulus onwards, you must answer whether the current stimulus matches the previous one in LOCATION ?',\n",
    "#                'ctxcol': 'If stimuli 1 and 2 match in CATEGORY, match stimuli 2 and 3 based on IDENTITY, otherwise on LOCATION.',\n",
    "#                'interdmsctgABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "            #    }\n",
    "\n",
    "\n",
    "\n",
    "# ins_mapping = {\n",
    "#                '1backloc': 'In this task, you will see sequences of six stimuli. \\n From the second stimulus onwards, you must answer whether the current stimulus matches the previous one in LOCATION ?',\n",
    "#                '1backctg': 'In this task, you will see sequences of six stimuli. \\n From the second stimulus onwards, you must answer whether the current stimulus matches the previous one in CATEGORY ?',\n",
    "#                '1backobj': 'In this task, you will see sequences of six stimuli. \\n From the second stimulus onwards, you must answer whether the current stimulus matches the previous one in IDENTITY ?',\n",
    "#                'ctxlco': 'If stimuli 1 and 2 match in LOCATION, match stimuli 2 and 3 based on CATEGORY, otherwise on IDENTITY.',\n",
    "#                'ctxcol': 'If stimuli 1 and 2 match in CATEGORY, match stimuli 2 and 3 based on IDENTITY, otherwise on LOCATION.',\n",
    "#                'interdmslocABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on LOCATION. Respond as fast as you can',\n",
    "#                'interdmsctgABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "#                'interdmsobjABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "#                'interdmslocABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on LOCATION. Respond as fast as you can',\n",
    "#                'interdmsctgABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "#                'interdmsobjABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "#                }\n",
    "\n",
    "\n",
    "ins_mapping = {\n",
    "               '1backloc': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on LOCATION. Respond as fast as you can',\n",
    "               '1backctg': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on CATEGORY. Respond as fast as you can',\n",
    "               '1backobj': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on IDENTITY. Respond as fast as you can',\n",
    "               'ctxlco': 'If stimuli 1 and 2 match in LOCATION, match stimuli 2 and 3 based on CATEGORY, otherwise on IDENTITY.',\n",
    "               'ctxcol': 'If stimuli 1 and 2 match in CATEGORY, match stimuli 2 and 3 based on IDENTITY, otherwise on LOCATION.',\n",
    "               'interdmslocABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on LOCATION. Respond as fast as you can',\n",
    "               'interdmsctgABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "               'interdmsobjABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "               'interdmslocABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on LOCATION. Respond as fast as you can',\n",
    "               'interdmsctgABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "               'interdmsobjABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "               }\n",
    "\n",
    "# ins_mapping = {\n",
    "#                '1backloc': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on LOCATION. Respond as fast as you can',\n",
    "#                '1backctg': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on CATEGORY. Respond as fast as you can',\n",
    "#                '1backobj': 'Match stimuli 1 and 2, 2 and 3, 3 and 4, 4 and 5, 5 and 6 based on IDENTITY. Respond as fast as you can',\n",
    "#                'interdmslocABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on LOCATION. Respond as fast as you can',\n",
    "#                'interdmsctgABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "#                'interdmsobjABBA': 'Match stimuli 2 and 3, then stimuli 1 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "#                'interdmslocABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on LOCATION. Respond as fast as you can',\n",
    "#                'interdmsctgABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on CATEGORY. Respond as fast as you can',\n",
    "#                'interdmsobjABAB': 'Match stimuli 1 and 3, and stimuli 2 and 4 based on IDENTITY. Respond as fast as you can',\n",
    "#                }\n",
    "\n",
    "tasks = ['1backloc', '1backctg', '1backobj', 'ctxlco' , 'ctxcol', \n",
    "       'interdmsobjABAB', 'interdmslocABBA', 'interdmslocABAB',\n",
    "       'interdmsctgABAB', 'interdmsobjABBA','interdmsctgABBA'] \n",
    "\n",
    "# tasks = ['1backloc', '1backctg', '1backobj', \n",
    "#        'interdmsobjABAB', 'interdmslocABBA', 'interdmslocABAB',\n",
    "#        'interdmsctgABAB', 'interdmsobjABBA','interdmsctgABBA'] \n",
    "\n",
    "task_features= {'loc':['1backloc', 'interdmslocABAB', 'interdmslocABBA'], \n",
    "                 'ctg':['1backctg', 'interdmsctgABAB','interdmsctgABBA' ], \n",
    "                 'obj':['1backobj', 'interdmsobjABAB', 'interdmsobjABBA']}\n",
    "\n",
    "# tasks = ['ctxcol', 'ctxlco' ,\n",
    "#        'interdmsobjABAB', 'interdmslocABBA', 'interdmslocABAB',\n",
    "#        'interdmsctgABAB', 'interdmsobjABBA','interdmsctgABBA'] \n",
    "\n",
    "# tasks = task_features['ctg']\n",
    "\n",
    "# sort tasks using ins_mapping dictionary keys\n",
    "tasks.sort(key=lambda x: list(ins_mapping.keys()).index(x))\n",
    "\n",
    "def pairwise_pearsonr(data):\n",
    "    num_vars = data.shape[0]\n",
    "    correlations = np.zeros((num_vars, num_vars))\n",
    "    \n",
    "    for i in range(num_vars):\n",
    "        for j in range(num_vars):\n",
    "            if i == j:\n",
    "                correlations[i, j] = 1.0  # Diagonal elements (correlation with itself) are always 1\n",
    "            else:\n",
    "                correlations[i, j], _ = pearsonr(data[i, : ], data[j, :])\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "def sentence_similarity(sentences, model):\n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "    \n",
    "\n",
    "    # Compute cosine-similarities for each sentence with each other sentence\n",
    "    scores = util.cos_sim(embeddings, embeddings)\n",
    "    # embeddings = embeddings.cpu().numpy()\n",
    "    # scores =  np.log(euclidean_distances(embeddings, embeddings))\n",
    "\n",
    "    # pearsonr_scores = np.array([])\n",
    "    # # Compute pearson correlation over all sentence embeddings\n",
    "    # for i in range(embeddings.shape[0]):\n",
    "    #     pearsonr_score = np.ones(2,) - pearsonr(embeddings[i], embeddings[i])\n",
    "    #     pearsonr_scores = np.append(pearsonr_scores, pearsonr_score)\n",
    "\n",
    "    # print(pearsonr_scores.reshape(embeddings.shape[0], embeddings.shape[0]))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "instructions = [ins for name,ins in list(ins_mapping.items()) if name in tasks]\n",
    "names = [name for name,ins in list(ins_mapping.items()) if name in tasks]\n",
    "\n",
    "setence_rsm = sentence_similarity(instructions, model).cpu().numpy()\n",
    "\n",
    "# Create a heatmap from the RSM with ticklabels as sentences\n",
    "plt.imshow(setence_rsm, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.yticks(range(len(names)), names, )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stimuli mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = len(tasks)\n",
    "n_feature_bins = 3 + 3*2 # = 9\n",
    "\n",
    "# Empty Mask\n",
    "\n",
    "loc_mask = np.zeros((n_feature_bins, n_feature_bins))\n",
    "cat_mask = np.zeros((n_feature_bins, n_feature_bins))\n",
    "obj_mask = np.zeros((n_feature_bins, n_feature_bins))\n",
    "\n",
    "# Fill in the loc mask\n",
    "loc_mask[0, 0] = 1\n",
    "loc_mask[0, 3] = 1\n",
    "loc_mask[3, 0] = 1\n",
    "loc_mask[0, 6] = 1\n",
    "loc_mask[6, 0] = 1\n",
    "\n",
    "loc_mask[3, 3] = 1\n",
    "loc_mask[3, 6] = 1\n",
    "loc_mask[6, 3] = 1\n",
    "loc_mask[6, 6] = 1\n",
    "\n",
    "# Fill in the cat mask\n",
    "cat_mask[1, 1] = 1\n",
    "cat_mask[1, 4] = 1\n",
    "cat_mask[4, 1] = 1\n",
    "cat_mask[1, 7] = 1\n",
    "cat_mask[7, 1] = 1\n",
    "\n",
    "cat_mask[4, 4] = 1\n",
    "cat_mask[4, 7] = 1\n",
    "cat_mask[7, 4] = 1\n",
    "cat_mask[7, 7] = 1\n",
    "\n",
    "# Fill in the obj mask\n",
    "obj_mask[2, 2] = 1\n",
    "obj_mask[2, 5] = 1\n",
    "obj_mask[5, 2] = 1\n",
    "obj_mask[2, 8] = 1\n",
    "obj_mask[8, 2] = 1\n",
    "\n",
    "obj_mask[5, 5] = 1\n",
    "obj_mask[5, 8] = 1\n",
    "obj_mask[8, 5] = 1\n",
    "obj_mask[8, 8] = 1\n",
    "\n",
    "# Add all masks together\n",
    "mask = loc_mask + cat_mask + obj_mask\n",
    "\n",
    "# Plot the mask\n",
    "plt.imshow(mask, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.yticks(range(len(names)), names, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tasks = len(tasks)\n",
    "\n",
    "# Empty Mask\n",
    "\n",
    "oneback_mask = np.zeros((n_tasks, n_tasks))\n",
    "interdms_mask = np.zeros((n_tasks, n_tasks))\n",
    "\n",
    "\n",
    "# Fill in the oneback mask\n",
    "oneback_mask[0:3, 0:3] = 1\n",
    "\n",
    "# Fill in the interdms mask\n",
    "interdms_mask[3:9, 3:9] = 1\n",
    "\n",
    "# Add all masks together\n",
    "mask = oneback_mask + interdms_mask\n",
    "\n",
    "# Plot the mask\n",
    "plt.imshow(mask, cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.yticks(range(len(names)), names, )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brain Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import surfplot\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read roi labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters + data # data/data/glasser_data_glm_then_concat/glm_betas/sub-03/correct_ses5_task-interdmsctgABBA_run-02.csv\n",
    "subj = 'sub-03'\n",
    "runs = ['run-01', 'run-02', 'run-03', 'run-04', 'run-05']\n",
    "sessions = ['ses1', 'ses2', 'ses3', 'ses4', 'ses5', 'ses6', 'ses7', 'ses8', 'ses9', 'ses10', 'ses11', 'ses12', 'ses13', 'ses14', 'ses15', 'ses16']\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Get the path of the parent directory\n",
    "parent_dir = '/Users/lucasgomez/Desktop/Neuro/Bashivan/Misc/Hackthon_WM_fMRI' # os.path.abspath(os.path.join(cwd, os.pardir))\n",
    "\n",
    "basedir = parent_dir\n",
    "datadir = basedir + '/data/data/glm_betas_encoding_delay_TR_betas/' + subj + '/'\n",
    "\n",
    "glasser_atlas_str= basedir + '/data/Glasser_LR_Dense64k.dlabel.nii'\n",
    "glasser_atlas = nib.load(glasser_atlas_str).get_fdata()[0].astype(int)\n",
    "print(glasser_atlas.shape)\n",
    "num_regions = 360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_betas = {}\n",
    "df_conditions = {} # 'task-dmsloc_run-01'\n",
    "\n",
    "for task in tasks:\n",
    "    for sess in sessions:\n",
    "        for run in runs:\n",
    "            try:\n",
    "                h5f = h5py.File(datadir + 'normalized_correct_glmmethod1_' + sess + '_' + 'task-' + task + '_' + run + '_betas.h5','r')\n",
    "                try: \n",
    "                    task_betas[task][sess][run] = h5f['betas'][:].copy()\n",
    "                    df_conditions[task][sess][run] = pd.read_csv(datadir + 'correct_glmmethod1_' + sess + '_' + 'task-' + task + '_' + run + '.csv')\n",
    "                except Exception as e:\n",
    "                    try:\n",
    "                        task_betas[task][sess] = {}\n",
    "                        task_betas[task][sess][run] = h5f['betas'][:].copy()\n",
    "                        df_conditions[task][sess] = {}\n",
    "                        df_conditions[task][sess][run] = pd.read_csv(datadir + 'correct_glmmethod1_' + sess + '_' + 'task-' + task + '_' + run + '.csv')\n",
    "                    except Exception as e:\n",
    "                        task_betas[task] = {}\n",
    "                        task_betas[task][sess] = {}\n",
    "                        task_betas[task][sess][run] = h5f['betas'][:].copy()\n",
    "                        df_conditions[task]= {}\n",
    "                        df_conditions[task][sess] = {}\n",
    "                        df_conditions[task][sess][run] = pd.read_csv(datadir + 'correct_glmmethod1_' + sess + '_' + 'task-' + task + '_' + run + '.csv')\n",
    "                h5f.close()\n",
    "            except Exception as e:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolate first delay betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_task_betas = task_betas.copy()\n",
    "\n",
    "for task in task_betas.keys():\n",
    "    for sess in task_betas[task].keys():\n",
    "        for run in task_betas[task][sess].keys():\n",
    "            task_df = df_conditions[task][sess][run]\n",
    "            # Filter task_df based on task_df['prev_stimulus'] == 1000  and task_df['regressor_type'] == 'encoding'\n",
    "            filtered_df = task_df[(task_df['prev_stimulus'] == 1000) & (task_df['regressor_type'] == 'encoding')]\n",
    "            betas  = task_betas[task][sess][run]\n",
    "            try:\n",
    "                # Divide all values of filtered_df.loc[:, 0] by 2\n",
    "                filtered_df.loc[:, 'Unnamed: 0'] = filtered_df.loc[:, 'Unnamed: 0'] // 2\n",
    "                print(max(filtered_df.loc[:, 'Unnamed: 0']))\n",
    "                print(betas.shape)\n",
    "                betas = betas[:, filtered_df['Unnamed: 0'].to_numpy()]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                # print('Error in task: ', task, ' sess: ', sess, ' run: ', run)\n",
    "\n",
    "            filtered_task_betas[task][sess][run] = betas\n",
    "\n",
    "task_betas = filtered_task_betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df_lh = pd.read_excel(basedir + '/data/Glasser_2016_Table.xlsx')\n",
    "map_df_lh = map_df_lh.iloc[:,0:2]\n",
    "\n",
    "# Insert row with region 0\n",
    "map_df_lh.loc[-1] = [0, 'LG']\n",
    "map_df_lh.index = map_df_lh.index + 1  # shifting index\n",
    "map_df_lh.sort_index(inplace=True)\n",
    "\n",
    "# Rename columns\n",
    "map_df_lh.columns = ['region_id', 'region_name']\n",
    "\n",
    "map_df_rh = map_df_lh.copy()\n",
    "\n",
    "# Increase map_df_rh region_id by 180 and index by 180\n",
    "map_df_rh['region_id'] = map_df_rh['region_id'] + 180\n",
    "map_df_rh.index = map_df_rh.index + 180\n",
    "\n",
    "# Concatenate both hemispheres\n",
    "map_df = pd.concat([map_df_lh, map_df_rh], axis=0)\n",
    "\n",
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values in the region_id column\n",
    "map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_atlas = {}\n",
    "\n",
    "for id, name in zip(map_df['region_id'], map_df['region_name']):\n",
    "    # indexes of id in glasser_atlas\n",
    "    if str(name) not in mapped_atlas.keys():\n",
    "        mapped_atlas[str(name)] = np.where(glasser_atlas == id)[0]\n",
    "    else:\n",
    "        mapped_atlas[str(name)] = np.concatenate((mapped_atlas[str(name)], np.where(glasser_atlas == id)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avg Betas Across Sessions, Runs, and Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_betas_dict = {}\n",
    "\n",
    "# Across each region\n",
    "for name, indexes in mapped_atlas.items():\n",
    "    avg_betas_dict[name] = {}\n",
    "\n",
    "    # Across each task\n",
    "    for task in task_betas.keys():\n",
    "        sess_betas = torch.tensor([])\n",
    "\n",
    "        # Across each session\n",
    "        for sess in task_betas[task].keys():\n",
    "            run_betas = torch.tensor([])\n",
    "\n",
    "            # Across each run\n",
    "            for run in task_betas[task][sess].keys():\n",
    "                # Betas for all trials\n",
    "                betas = torch.tensor(task_betas[task][sess][run][indexes])\n",
    "\n",
    "                # Check if betas is all zeros\n",
    "                if torch.sum(betas) == 0 and name != 'LG':\n",
    "                    print('Found all zeros for ' + name + ' ' + task + ' ' + sess + ' ' + run)\n",
    "                    continue\n",
    "\n",
    "                # Mean beta across trials\n",
    "                trial_avg_betas = torch.mean(betas, axis=1).unsqueeze(1)\n",
    "\n",
    "                # Add to run_betas\n",
    "                run_betas = torch.cat((run_betas, trial_avg_betas), axis=1)\n",
    "            \n",
    "            # Check if run_betas is not empty\n",
    "            if run_betas.shape[0] != 0:\n",
    "                run_betas = torch.mean(run_betas, axis=1).unsqueeze(1)\n",
    "                sess_betas = torch.cat((sess_betas, run_betas), axis=1)\n",
    "            else:\n",
    "                print('no data at: ', name, task, sess, run)\n",
    "        sess_betas = torch.mean(sess_betas, axis=1)  \n",
    "        avg_betas_dict[name][task] = sess_betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_betas_dict['V1']['1backloc'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make RSMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_rsms = {}\n",
    "for name in avg_betas_dict.keys():\n",
    "    betas = torch.stack(list(avg_betas_dict[name].values()))\n",
    "    brain_rsms[name] = util.cos_sim(betas, betas).numpy() #   np.log(euclidean_distances(betas, betas)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 'PFt'\n",
    "\n",
    "# Plot v1_betas_rdm with ticklabels as sentences\n",
    "plt.imshow(brain_rsms[area], cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "_ = plt.xticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys(), rotation=90)\n",
    "_ = plt.yticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all regions\n",
    "for area in brain_rsms.keys():\n",
    "  \n",
    "    plt.clf()\n",
    "    plt.imshow(brain_rsms[area], cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    _ = plt.xticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys(), rotation=90)\n",
    "    _ = plt.yticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys())\n",
    "    plt.savefig('/Users/lucasgomez/Desktop/Neuro/Bashivan/Misc/Hackthon_WM_fMRI/tasks/4.2 - NLP Similarity/figures/' + area + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare RDMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compare_rsms(rdm1, rdm2):\n",
    "    # Flatten the RSMs\n",
    "    rdm1_flat = rdm1.flatten()\n",
    "    rdm2_flat = rdm2.flatten()\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    correlation, _ = pearsonr(rdm1_flat, rdm2_flat)\n",
    "    \n",
    "    return correlation\n",
    "\n",
    "rsms_comp = {}\n",
    "for name in brain_rsms.keys():\n",
    "    # Get upper triangular part of the RSMs\n",
    "    triu_setence_rsm =setence_rsm[np.triu_indices(setence_rsm.shape[0], k=1)]  #  mask[np.triu_indices(mask.shape[0], k=1)] # \n",
    "    triu_brain_rsm = brain_rsms[name][np.triu_indices(brain_rsms[name].shape[0], k=1)]\n",
    "\n",
    "\n",
    "    sim = compare_rsms(triu_setence_rsm, triu_brain_rsm)\n",
    "    if sim > 0.9995:\n",
    "        print(name, sim)\n",
    "    rsms_comp[name] = sim\n",
    "\n",
    "# Create a bar plot of the RSMs comparison\n",
    "# plt.bar(rsms_comp.keys(), rsms_comp.values())\n",
    "rsms_comp['LG'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the rsms_comp dictionary by value\n",
    "rsms_comp = dict(sorted(rsms_comp.items(), key=lambda item: item[1]))\n",
    "\n",
    "# Print the top 5 dictionary values along with their key\n",
    "for key in list(rsms_comp.keys())[-10:]:\n",
    "    print(key, rsms_comp[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(list(rsms_comp.values())[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromaps.datasets import fetch_fslr\n",
    "\n",
    "# Fetch the fsLR atlas\n",
    "surfaces = fetch_fslr()\n",
    "lh, rh = surfaces['veryinflated']\n",
    "surface_dat = np.zeros((len(glasser_atlas),))\n",
    "\n",
    "count_regs = 0\n",
    "\n",
    "# Create a surface data array\n",
    "for name, indexes in mapped_atlas.items():\n",
    "\n",
    "    print('name: ', name)\n",
    "    print([rsms_comp[name]])\n",
    "    count_regs = count_regs + len(indexes)\n",
    "    sim_repeated = np.array([rsms_comp[name]]*len(indexes))\n",
    "\n",
    "    surface_dat[indexes] = np.nan_to_num(sim_repeated)\n",
    "\n",
    "p = surfplot.Plot(lh,rh,size=(1000,750),zoom=1.8)\n",
    "p.add_layer(surface_dat.T,cmap='magma',color_range=[np.min(surface_dat), np.max(surface_dat)])  \n",
    "fig = p.build(figsize=(5,5),colorbar=True,cbar_kws={'fontsize':6})\n",
    "fig.suptitle('task instruction \\nbrain similarity',y=0.88,fontsize=10)\n",
    "fig.tight_layout()\n",
    "fig.savefig('figures/task_ins_similarity.png',dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuromaps.datasets import fetch_fslr\n",
    "\n",
    "# Fetch the fsLR atlas\n",
    "surfaces = fetch_fslr()\n",
    "lh, rh = surfaces['veryinflated']\n",
    "surface_dat = np.zeros((len(glasser_atlas),))\n",
    "\n",
    "count_regs = 0\n",
    "\n",
    "# Create a surface data array\n",
    "for name, indexes in mapped_atlas.items():\n",
    "    count_regs = count_regs + len(indexes)\n",
    "    sim_repeated = np.array([rsms_comp[name]]*len(indexes))\n",
    "\n",
    "    if name in ['44']:\n",
    "        surface_dat[indexes] = np.nan_to_num(sim_repeated)\n",
    "    \n",
    "    if name in ['V1']:\n",
    "        surface_dat[indexes] = surface_dat[indexes] = np.array([torch.tensor(0.0, dtype=torch.float64)]*len(indexes))\n",
    "\n",
    "\n",
    "p = surfplot.Plot(lh,rh,size=(1000,750),zoom=1.8)\n",
    "p.add_layer(surface_dat.T,cmap='magma',color_range=[np.min(surface_dat), np.max(surface_dat)])  \n",
    "fig = p.build(figsize=(5,5),colorbar=True,cbar_kws={'fontsize':6})\n",
    "fig.suptitle('task instruction \\nbrain similarity',y=0.88,fontsize=10)\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_betas_dict_split_1 = {}\n",
    "avg_betas_dict_split_2 = {}\n",
    "\n",
    "# Across each region\n",
    "for name, indexes in mapped_atlas.items():\n",
    "    avg_betas_dict_split_1[name] = {}\n",
    "    avg_betas_dict_split_2[name] = {}\n",
    "\n",
    "    # Across each task\n",
    "    for task in task_betas.keys():\n",
    "        sess1_betas = torch.tensor([])\n",
    "        sess2_betas = torch.tensor([])\n",
    "\n",
    "        # # Split sessions into two groups\n",
    "        sessions = list(task_betas[task].keys())\n",
    "        # random.shuffle(sessions)\n",
    "        sessions_1 = sessions[:len(sessions)//2]\n",
    "        sessions_2 = sessions[len(sessions)//2:]\n",
    "\n",
    "        # Split sessions into two random groups\n",
    "\n",
    "\n",
    "        # Make them equal\n",
    "        if len(sessions) % 2 != 0:\n",
    "            sessions_2 = sessions_2[:-1]\n",
    "\n",
    "        for sess in sessions_1:\n",
    "            run_betas = torch.tensor([])\n",
    "\n",
    "            # Across each run\n",
    "            for run in task_betas[task][sess].keys():\n",
    "                # Betas for all trials\n",
    "                betas = torch.tensor(task_betas[task][sess][run][indexes])\n",
    "\n",
    "                # Check if betas is all zeros\n",
    "                if torch.sum(betas) == 0 and name != 'LG':\n",
    "                    print('Found all zeros for ' + name + ' ' + task + ' ' + sess + ' ' + run)\n",
    "                    continue\n",
    "\n",
    "\n",
    "                # Mean beta across trials\n",
    "                trial_avg_betas = torch.mean(betas, axis=1).unsqueeze(1)\n",
    "\n",
    "                # Add to run_betas\n",
    "                run_betas = torch.cat((run_betas, trial_avg_betas), axis=1)\n",
    "            \n",
    "            # Check if run_betas is not empty\n",
    "            if run_betas.shape[0] != 0:\n",
    "                run_betas = torch.mean(run_betas, axis=1).unsqueeze(1)\n",
    "                sess1_betas = torch.cat((sess1_betas, run_betas), axis=1)\n",
    "            else:\n",
    "                print('no data at: ', name, task, sess, run)\n",
    "\n",
    "        sess1_betas = torch.mean(sess1_betas, axis=1)  \n",
    "        avg_betas_dict_split_1[name][task] = sess1_betas\n",
    "\n",
    "\n",
    "        for sess in sessions_2:\n",
    "            run_betas = torch.tensor([])\n",
    "\n",
    "            # Across each run\n",
    "            for run in task_betas[task][sess].keys():\n",
    "                # Betas for all trials\n",
    "                betas = torch.tensor(task_betas[task][sess][run][indexes])\n",
    "\n",
    "                # Check if betas is all zeros\n",
    "                if torch.sum(betas) == 0 and name != 'LG':\n",
    "                    print('Found all zeros for ' + name + ' ' + task + ' ' + sess + ' ' + run)\n",
    "                    continue\n",
    "                \n",
    "                # Mean beta across trials\n",
    "                trial_avg_betas = torch.mean(betas, axis=1).unsqueeze(1)\n",
    "\n",
    "                # Add to run_betas\n",
    "                run_betas = torch.cat((run_betas, trial_avg_betas), axis=1)\n",
    "            \n",
    "            # Check if run_betas is not empty\n",
    "            if run_betas.shape[0] != 0:\n",
    "                run_betas = torch.mean(run_betas, axis=1).unsqueeze(1)\n",
    "                sess2_betas = torch.cat((sess2_betas, run_betas), axis=1)\n",
    "            else:\n",
    "                print('no data at: ', name, task, sess, run)\n",
    "\n",
    "        sess2_betas = torch.mean(sess2_betas, axis=1)  \n",
    "        avg_betas_dict_split_2[name][task] = sess2_betas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brain_split_rsms = {}\n",
    "for name in avg_betas_dict_split_1.keys():\n",
    "    betas_split1 = torch.stack(list(avg_betas_dict_split_1[name].values()))\n",
    "    betas_split2 = torch.stack(list(avg_betas_dict_split_2[name].values()))\n",
    "    brain_split_rsms[name] = util.cos_sim(betas_split2, betas_split2).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = 'PFt'\n",
    "\n",
    "# Plot v1_betas_rdm with ticklabels as sentences\n",
    "plt.imshow(brain_split_rsms[area], cmap='hot', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "_ = plt.xticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys(), rotation=90)\n",
    "_ = plt.yticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all regions\n",
    "for area in brain_rsms.keys():\n",
    "  \n",
    "    plt.clf()\n",
    "    plt.imshow(brain_split_rsms[area], cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    _ = plt.xticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys(), rotation=90)\n",
    "    _ = plt.yticks(range(len(avg_betas_dict[area].keys())), avg_betas_dict[area].keys())\n",
    "    plt.savefig('/Users/lucasgomez/Desktop/Neuro/Bashivan/Misc/Hackthon_WM_fMRI/tasks/4.2 - NLP Similarity/figures/cross_val/' + area + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
